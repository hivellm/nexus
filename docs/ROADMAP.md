# Nexus Development Roadmap

This document outlines the phased implementation plan for Nexus graph database.

## Status Legend

- âœ… Completed
- ðŸš§ In Progress
- ðŸ“‹ Planned
- ðŸ”® Future (Post V2)

---

## Phase 0: Foundation (Current)

**Goal**: Architecture documentation and project scaffolding

### Tasks

- âœ… Architecture design documentation
- âœ… Cargo workspace setup (edition 2024, nightly)
- âœ… Core module scaffolding (nexus-core)
- âœ… Server scaffolding (nexus-server)
- âœ… Protocol layer scaffolding (nexus-protocol)
- ðŸ“‹ Integration tests structure
- ðŸ“‹ CI/CD workflows (test, lint, coverage)

**Deliverable**: Buildable workspace with comprehensive documentation

---

## Phase 1: MVP - Single Node Engine

**Goal**: Working graph database with basic Cypher and native KNN

**Timeline**: 8-12 weeks

### 1.1 Storage Foundation (Week 1-2) âœ… COMPLETED

- âœ… **Catalog Implementation**
  - LMDB/heed integration (10GB max, 8 databases)
  - Label/Type/Key â†’ ID bidirectional mappings
  - Statistics storage (node counts, relationship counts)
  - Schema metadata (version, epoch, page_size)
  - 21 tests, 98.64% coverage

- âœ… **Record Stores**
  - nodes.store: Fixed 32-byte records with memmap2
  - rels.store: Fixed 48-byte records with linked lists
  - Automatic file growth (1MB â†’ 2x exponential)
  - Label bitmap (64 labels per node)
  - Doubly-linked adjacency lists for O(1) traversal
  - 18 tests, 96.96% coverage

- âœ… **Page Cache**
  - 8KB pages with Clock eviction algorithm
  - Pin/unpin semantics with atomic reference counting
  - Dirty page tracking with HashSet
  - xxHash3 checksums for corruption detection
  - Statistics (hits, misses, evictions, hit rate)
  - 21 tests, 96.15% coverage

**Test Coverage**: âœ… 96%+ for storage layer (exceeds 95% requirement)

### 1.2 Durability & Transactions (Week 3-4) âœ… COMPLETED

- âœ… **Write-Ahead Log (WAL)**
  - Append-only log with 10 entry types
  - CRC32 validation for data integrity
  - fsync on commit for durability
  - Checkpoint mechanism with statistics
  - WAL replay for crash recovery
  - Truncate operation for WAL rotation
  - 16 tests, 96.71% coverage

- âœ… **MVCC Implementation**
  - Epoch-based snapshots for snapshot isolation
  - Single-writer model (parking_lot Mutex)
  - Begin/commit/abort transaction logic
  - Visibility rules (created_epoch <= tx_epoch < deleted_epoch)
  - Transaction statistics tracking
  - 20 tests, 99.02% coverage

- âœ… **Integration Tests**
  - 15 end-to-end tests covering all modules
  - Performance benchmarks (>100K reads/sec, >10K writes/sec)
  - Crash recovery validation
  - MVCC snapshot isolation verification
  - Concurrent access (5 readers + 3 writers)

**Test Coverage**: âœ… 96.06% global coverage (exceeds 95% requirement)
**Total Tests**: âœ… 133 tests (118 unit + 15 integration), 100% passing

### 1.3 Basic Indexes (Week 5)

- ðŸ“‹ **Label Bitmap Index**
  - RoaringBitmap per label
  - Add/remove node from label
  - Cardinality statistics
  - Bitmap operations (AND, OR, NOT)

- ðŸ“‹ **KNN Vector Index**
  - HNSW index per label (hnsw_rs)
  - node_id â†’ embedding_idx mapping
  - Cosine similarity support
  - Configurable M, ef_construction parameters

**Test Coverage**: 95%+ with benchmark comparisons

### 1.4 Cypher Executor (Week 6-8)

- ðŸ“‹ **Parser**
  - Basic pattern syntax: `(n:Label)-[r:TYPE]->(m)`
  - WHERE predicates: `=`, `>`, `<`, `>=`, `<=`, `!=`, `AND`, `OR`
  - RETURN projection
  - ORDER BY + LIMIT
  - Simple aggregations: COUNT, SUM, AVG, MIN, MAX

- ðŸ“‹ **Physical Operators**
  - NodeByLabel: Scan label bitmap
  - Filter: Property predicates (vectorized if possible)
  - Expand: Traverse linked lists (OUT, IN, BOTH)
  - Project: Return expressions
  - OrderBy + Limit: Top-K heap
  - Aggregate: Hash aggregation

- ðŸ“‹ **Query Planner**
  - Heuristic cost-based planning
  - Pattern reordering by selectivity
  - Index selection (label bitmap vs KNN)
  - Filter pushdown

**Test Coverage**: 95%+ with TPC-like graph queries

### 1.5 HTTP API (Week 9)

- ðŸ“‹ **REST Endpoints**
  - POST /cypher: Execute queries
  - POST /knn_traverse: KNN-seeded traversal
  - POST /ingest: Bulk data loading
  - GET /health: Health check
  - GET /stats: Database statistics

- ðŸ“‹ **Streaming Support**
  - Server-Sent Events (SSE) for large results
  - Chunked transfer encoding
  - Backpressure handling

**Test Coverage**: 95%+ with integration tests

### 1.6 MVP Integration & Testing (Week 10-12)

- ðŸ“‹ **End-to-End Tests**
  - Sample graph datasets (social network, knowledge graph)
  - Cypher query test suite
  - KNN + traversal hybrid queries
  - Crash recovery scenarios

- ðŸ“‹ **Performance Benchmarks**
  - Point reads: 100K+ ops/sec target
  - KNN queries: 10K+ ops/sec target
  - Pattern traversal: 1K-10K ops/sec
  - Bulk ingest: 100K+ nodes/sec

- ðŸ“‹ **Documentation**
  - User guide with examples
  - API reference (OpenAPI spec)
  - Deployment guide
  - Performance tuning guide

**MVP Deliverable**: Production-ready single-node graph database with native KNN

---

## Phase 2: V1 - Quality & Features

**Goal**: Advanced indexes, constraints, optimization

**Timeline**: 6-8 weeks

### 2.1 Advanced Indexes (Week 13-14)

- ðŸ“‹ **Property B-tree Index**
  - Composite key: (label_id, key_id, value)
  - Range queries: `WHERE n.age > 18`
  - Unique constraints enforcement
  - Index statistics (NDV, histograms)

- ðŸ“‹ **Full-Text Search**
  - Tantivy integration per (label, key)
  - BM25 scoring
  - Fuzzy search, phrase queries
  - CALL text.search() procedure

**Test Coverage**: 95%+ with query performance tests

### 2.2 Constraints & Schema (Week 15-16)

- ðŸ“‹ **Constraint Types**
  - UNIQUE(label, key): Enforce uniqueness
  - NOT NULL(label, key): Required properties
  - CHECK(predicate): Custom validation
  - FOREIGN KEY: Relationship integrity (optional)

- ðŸ“‹ **Schema Evolution**
  - ADD/DROP constraint migrations
  - Index creation/deletion
  - Online schema changes (background)

**Test Coverage**: 95%+ with constraint violation tests

### 2.3 Query Optimization (Week 17-18)

- ðŸ“‹ **Advanced Planner**
  - Cost model with actual statistics
  - Join order optimization
  - Index-only scans where possible
  - Common subexpression elimination

- ðŸ“‹ **Query Cache**
  - Prepared statement caching
  - Result caching for read-only queries
  - Cache invalidation on writes

- ðŸ“‹ **Execution Optimizations**
  - Vectorized filter evaluation (SIMD)
  - Parallel execution (thread pool)
  - Pipelining operators
  - Lazy materialization

**Test Coverage**: 95%+ with benchmark regressions

### 2.4 Bulk Loader (Week 19)

- ðŸ“‹ **Direct Store Generation**
  - Bypass WAL for initial load
  - Sort nodes/relationships for locality
  - Build indexes during load
  - Parallel ingestion

- ðŸ“‹ **Import Formats**
  - CSV import (Neo4j compatible)
  - NDJSON import
  - GraphML import
  - Custom binary format

**Test Coverage**: 95%+ with large dataset tests

### 2.5 Authentication & Security (Week 20)

- ðŸ“‹ **API Key Authentication**
  - API key generation and management
  - Argon2 hashing for security
  - Storage in catalog (LMDB)
  - Disabled by default, required for 0.0.0.0 binding

- ðŸ“‹ **Rate Limiting**
  - Per-API-key limits (1000/min, 10000/hour)
  - X-RateLimit-* headers
  - 429 Too Many Requests responses

- ðŸ“‹ **RBAC (Role-Based Access Control)**
  - Permissions: READ, WRITE, ADMIN, SUPER
  - User â†’ Roles â†’ Permissions
  - JWT token support
  - Audit logging

**Test Coverage**: 95%+ with security scenario tests

### 2.6 Replication System (Week 21-22)

- ðŸ“‹ **Master-Replica Architecture**
  - Async replication (default)
  - Sync replication (optional quorum)
  - Read-only replicas
  - WAL streaming to replicas

- ðŸ“‹ **Replication Protocol**
  - Full sync via snapshot transfer
  - Incremental sync via WAL stream
  - Circular replication log (1M operations)
  - Auto-reconnect with exponential backoff

- ðŸ“‹ **Failover Support**
  - Health monitoring (heartbeat)
  - Manual promotion (POST /replication/promote)
  - Automatic failover (V2)
  - Replication lag monitoring

**Test Coverage**: 95%+ with failover tests

### 2.7 Desktop GUI (Electron) (Week 23-25)

- ðŸ“‹ **GUI Foundation**
  - Electron app structure
  - Vue 3 + TailwindCSS
  - IPC communication with server
  - Auto-updater integration

- ðŸ“‹ **Graph Visualization**
  - Force-directed graph layout (D3.js/Cytoscape.js)
  - Node/relationship filtering
  - Property inspector
  - Interactive zoom/pan

- ðŸ“‹ **Query Interface**
  - Cypher editor (CodeMirror with syntax highlighting)
  - Query execution
  - Result table/graph view toggle
  - Query history and saved queries

- ðŸ“‹ **Management Features**
  - Schema browser
  - Index management
  - Backup/restore UI
  - Replication monitoring
  - Performance dashboard (Chart.js)

- ðŸ“‹ **KNN Search UI**
  - Text input with embedding generation
  - Visual similarity results
  - Hybrid query builder

**Test Coverage**: 95%+ with E2E GUI tests

### 2.8 Monitoring & Operations (Week 26)

- ðŸ“‹ **Metrics Exposure**
  - Prometheus metrics endpoint
  - Query latency histograms
  - Cache hit rates
  - WAL size, checkpoint frequency
  - Replication lag metrics

- ðŸ“‹ **Operational Tools**
  - Backup/restore utilities
  - Point-in-time snapshots
  - Database compaction
  - Index rebuild

**Test Coverage**: 95%+ with ops scenario tests

**V1 Deliverable**: Production-grade single-node database with replication, GUI, and advanced features

---

## Phase 3: V2 - Distributed Graph

**Goal**: Horizontal scalability via sharding and replication

**Timeline**: 12-16 weeks

### 3.1 Sharding Architecture (Week 21-24)

- ðŸ”® **Shard Management**
  - Hash(node_id) â†’ shard_id assignment
  - Relationships reside with source node
  - Cross-shard edge pointers
  - Shard metadata catalog

- ðŸ”® **Data Partitioning**
  - Balanced hash partitioning
  - Range partitioning (optional)
  - Rebalancing strategies
  - Hot shard detection

**Test Coverage**: 95%+ with multi-shard scenarios

### 3.2 Replication (Week 25-28)

- ðŸ”® **Raft Consensus**
  - openraft integration per shard
  - Leader election
  - Log replication
  - Snapshot transfer

- ðŸ”® **Read Replicas**
  - Followers serve read-only queries
  - WAL streaming to replicas
  - Causal consistency guarantees
  - Replica lag monitoring

**Test Coverage**: 95%+ with failover tests

### 3.3 Distributed Queries (Week 29-32)

- ðŸ”® **Query Coordinator**
  - Plan decomposition
  - Shard-aware planning
  - Pushdown optimization (filters, limits)
  - Cross-shard joins

- ðŸ”® **Execution Runtime**
  - Scatter/gather pattern
  - Streaming results aggregation
  - Partial failure handling
  - Timeout management

**Test Coverage**: 95%+ with distributed query tests

### 3.4 Cluster Operations (Week 33-36)

- ðŸ”® **Cluster Management**
  - Node discovery (gossip or static config)
  - Health checking
  - Rolling upgrades
  - Shard migration

- ðŸ”® **Disaster Recovery**
  - Multi-region replication
  - Cross-datacenter latency handling
  - Backup coordination
  - Restore across cluster

**Test Coverage**: 95%+ with chaos engineering tests

**V2 Deliverable**: Distributed graph database with multi-node scalability

---

## Phase 4: Future Enhancements (Post V2)

### Graph Algorithms

- ðŸ”® Shortest path (Dijkstra, A*)
- ðŸ”® PageRank, centrality measures
- ðŸ”® Community detection (Louvain)
- ðŸ”® Graph neural network integration

### Advanced Features

- ðŸ”® Temporal graph (valid-time versioning)
- ðŸ”® Geospatial indexes and queries
- ðŸ”® Graph streaming (Kafka/Pulsar ingestion)
- ðŸ”® Multi-tenancy and access control
- ðŸ”® Encryption at rest and in transit

### Analytics Integration

- ðŸ”® Apache Arrow integration
- ðŸ”® OLAP-style aggregations
- ðŸ”® Data export to data lakes
- ðŸ”® BI tool connectors

### AI/ML Integration

- ðŸ”® Graph embeddings (Node2Vec, GraphSAGE)
- ðŸ”® Link prediction models
- ðŸ”® Anomaly detection
- ðŸ”® Knowledge graph completion

---

## Success Metrics

### MVP Success Criteria

- âœ… 95%+ test coverage across all modules
- âœ… Zero known critical bugs
- âœ… 100K+ point reads/sec (single node)
- âœ… 10K+ KNN queries/sec
- âœ… Complete API documentation
- âœ… Sample applications (RAG, recommendation)

### V1 Success Criteria

- âœ… All advanced indexes operational
- âœ… Query optimization reduces latency by 50%+
- âœ… Bulk loader achieves 500K+ nodes/sec
- âœ… Production deployments (internal)

### V2 Success Criteria

- âœ… Linear read scalability (add nodes â†’ proportional throughput)
- âœ… < 5% overhead from distribution
- âœ… Automatic failover in < 30 seconds
- âœ… Production deployments (external customers)

---

## Resource Requirements

### MVP (Phase 1)

- **Team**: 1-2 engineers
- **Duration**: 8-12 weeks
- **Hardware**: Development machines (8+ cores, 16GB+ RAM)

### V1 (Phase 2)

- **Team**: 2-3 engineers
- **Duration**: 6-8 weeks
- **Hardware**: Same as MVP + test clusters

### V2 (Phase 3)

- **Team**: 3-4 engineers
- **Duration**: 12-16 weeks
- **Hardware**: Multi-node test clusters (3-5 nodes per cluster)

---

## Risk Management

### Technical Risks

| Risk | Mitigation |
|------|------------|
| Page cache complexity | Start with simple Clock algorithm, optimize later |
| MVCC performance overhead | Benchmark early, tune epoch GC frequency |
| Distributed consensus latency | Careful shard sizing, batch replication |
| Query planner accuracy | Collect statistics, A/B test plans |

### Schedule Risks

| Risk | Mitigation |
|------|------------|
| Feature creep | Strict adherence to MVP scope |
| Testing bottleneck | Automated testing from day 1 |
| Integration delays | Mock external dependencies early |

---

## Dependencies

### External Projects

- **Vectorizer**: KNN embeddings may come from Vectorizer MCP
- **Synap**: Optional pub/sub for graph change events
- **UMICP**: Protocol integration for multi-service graphs

### Internal Milestones

- MVP completion required before V1 start
- V1 production usage required before V2 start
- Each phase gated on 95%+ test coverage

---

## Version History

| Version | Date | Changes |
|---------|------|---------|
| 0.1.0 | 2024-10-24 | Initial roadmap, Phase 0 scaffolding complete |

---

## Next Steps

1. Complete Phase 0 (scaffolding + docs) âœ…
2. Begin Phase 1.1 (Storage Foundation)
3. Weekly sync meetings to track progress
4. Update this roadmap as phases complete

